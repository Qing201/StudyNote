{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EE P 590: Deep Learning for Embedded Real-time Intelligence \n",
    "\n",
    "\n",
    "# Homework 3  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eep590_utils import * "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Warm-up (40 points) \n",
    "\n",
    "###  Answer the following in your own words (read and understand the concepts, do not copy answers from the internet): \n",
    "**(a.) Explain why the depth-wise separable convolutions can achieve reasonable results while using fewer parameters compared to traditional convolutions. Also compare the representation power of a depth-wise separable convolution to that of a standard convolution if both had the same number of parameters.**\n",
    "\n",
    "**(b.) Comment on the quantization bit-widths (for a deep learning model) required to achieve good performance for speech/audio inputs and compare it to that for image inputs. Which type of input requires higher number of bits to achieve reasonable performance, and why?** \n",
    "\n",
    "**(c.) From an optimization perspective, why is it challenging to train a deep learning model with quantized weights and activations?** \n",
    "\n",
    "**(d.) What is the impact of low-rank projections on the generalization ability of a deep learning model? Explain your reasoning.**\n",
    " \n",
    "**(e.) What is the motive to deploy deep learning models on embedded/IoT devices? List the problems associated with sending the data from the IoT device to the cloud through wifi and performing the inference on the cloud**\n",
    "\n",
    "**(f.) In one or more sentences, and using sketches as appropriate, explain: SqueezeNet and MobileNet. What are the defining characteristic of these architectures? How do they improve upon their predecessors in terms of latency and memory consumption?**\n",
    "\n",
    "\n",
    "**(f.) Compare and contrast the different pruning techniques discussed in the lecture. Which technique would produce the  most optimal results? Which technique is the most practical? Explain your reasoning.**\n",
    "\n",
    "\n",
    "**(g.) Explain how Scree plots are useful in resource allocation and determining each layer's target rank.**\n",
    "\n",
    "**(h.) Compare and contrast the different quantization techniques discussed in the lectures, and comment on the strengths and limitations of each technique.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Low-rank approximation (30 points) \n",
    "\n",
    "In this section we will use low-rank approximations of the dense layers to reduce the memory consumption of a simple  LeNet architecture during inference. To make it simpler, we have divided this into subtasks:- \n",
    "\n",
    "- **(8 points) Subtask 1: Train a standard LeNet architercture on the Fashion MNIST dataset. Use the `load_data_fashion_mnist` function provided in the `eep590_utils.py` file for downloading and loading the dataset. The model architecture is defined below for your convenience, but you are free to use any other implementations of the same architecture. Use a set of reasonable hyper-parameters to obtain an accuracy of atleast 75 percentage on the training and testing sets.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 3x3 square conv kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)  # 5x5 image dimension\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, int(x.nelement() / x.shape[0]))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **(10 points) Subtask 2: Project the `self.fc1` and `self.fc2` weights to subspaces with ranks `r1` and `r2`.** \n",
    "- **(4 points) Plot a graph with the X-axis representing the %improvement in memory usage of the architure, and on Y-axis the test accuracy of the model. (Hint: play with `r1` and `r2`, and run the inference multiple times. You need not re-train the model)** \n",
    "- **(4 points) Plot a graph of the %improvement in memory usage vs. $\\beta$, which is the percentage of total variation explained in the low-rank subspace. (Hint: Refer to slide 42 of the lecture)**\n",
    "- **(4 points) Plot a graph of test accuracy vs. $\\beta$.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Sparsification (30 points) \n",
    "\n",
    "In this section we will attempt to sparsify the model trained in subtask 1 of previous section. \n",
    "\n",
    "- **Subtask 1: Plot a histogram of the distribution of all the weights in the model.** \n",
    "- **Subtask 2: Implement a module to clip all the weights that have a magntude less than a threshold (say $\\alpha$) to zero.**\n",
    "- **Subtask 3: Run inference for different levels of sparsity by varying $\\alpha$, and plot a graph of sparsity vs. test accuracy.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Improved RNN (35 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we will implement a different type of RNN, one with weighted residual connections, and try to analyze it's usefulness. \n",
    "\n",
    "Let $X = [x 1 , . . . , x^T ]$ be the input data where $x_t \\in R^D$ denotes the t-th step feature vector. Then,\n",
    "the goal of multi-class RNNs is to learn a function F : R D×T → {1, . . . , L} that predicts one of L\n",
    "classes for the given data point X. Standard RNN architecture has a provision to produce an output\n",
    "at every time step, but we focus on the setting where each data point is associated with a single label\n",
    "that is predicted at the end of the time horizon T . Standard RNN maintains a vector of hidden state\n",
    "$h_t ∈ R^D̂$ which captures temporal dynamics in the input data, i.e., $$h_t = f(W \\, x_t + U \\, h_{t-1} + b)$$\n",
    "\n",
    "Learning U, W in the above architecture is difficult as the gradient\n",
    "can have exponentially large (in T) condition number, which leads to problems like exploding or vanishing gradients. Instead, ImprovedRNN will use a simple weighted residual connection to perform updates of the hidden state $h_t$ as follows: $$ h̃_t = f(Wx_t + Uh_{t−1} + b)$$  $$h_t = \\alpha h̃_t + βh_{t−1} $$\n",
    "\n",
    "where 0 ≤ α, β ≤ 1 are trainable scalar weights.\n",
    "\n",
    "\n",
    "- **(20 points) Subtask 1: Implement the `ImprovedRNNCell` by extending the RNNCell defined in `eep590_utils.py`, which extends `nn.Module` in pytorch.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedRNNCell(RNNCell):\n",
    "    '''\n",
    "    ImprovedRNN Cell with both Full Rank and Low Rank Formulations\n",
    "    Has multiple activation functions for the gates\n",
    "    hidden_size = # hidden units\n",
    "    update_nonlinearity = nonlinearity for final rnn update\n",
    "    can be chosen from [tanh, sigmoid, relu, quantTanh, quantSigm]\n",
    "    wRank = rank of W matrix (creates two matrices if not None)\n",
    "    uRank = rank of U matrix (creates two matrices if not None)\n",
    "    wSparsity = intended sparsity of W matrix(ces)\n",
    "    uSparsity = intended sparsity of U matrix(ces)\n",
    "    The cell will not automatically sparsify. The user must invoke .sparsify() to hard threshold.\n",
    "    alphaInit = init for alpha, the update scalar\n",
    "    betaInit = init for beta, the weight for previous state\n",
    "    Basic architecture is like:\n",
    "    h_t^ = update_nl(Wx_t + Uh_{t-1} + B_h)\n",
    "    h_t = sigmoid(beta)*h_{t-1} + sigmoid(alpha)*h_t^\n",
    "    W and U can further parameterised into low rank version by\n",
    "    W = matmul(W_1, W_2) and U = matmul(U_1, U_2)\n",
    "    '''\n",
    "\n",
    "    def __init__(self, input_size, hidden_size,\n",
    "                 update_nonlinearity=\"tanh\", wRank=None, uRank=None,\n",
    "                 wSparsity=1.0, uSparsity=1.0, alphaInit=-3.0, betaInit=3.0,\n",
    "                 name=\"ImprovedRNN\"):\n",
    "        super(ImprovedRNNCell, self).__init__(input_size, hidden_size,\n",
    "                                           None, update_nonlinearity,\n",
    "                                           1, 1, 1, wRank, uRank, wSparsity,\n",
    "                                           uSparsity)\n",
    "        # TODO: Initialize all the required variables\n",
    "\n",
    "    @property\n",
    "    def name(self):\n",
    "        return self._name\n",
    "\n",
    "    @property\n",
    "    def cellType(self):\n",
    "        return \"ImprovedRNN\"\n",
    "\n",
    "    def forward(self, input, state):\n",
    "        # TODO: Implement the forward pass to compute the new_state\n",
    "        pass\n",
    "        return new_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **(10 points) Subtask 2: Explain how Improved RNN resolves some of the problems in traditional RNNs from an optimization perspective.** Assume that the label decision function is one dimensional and is given by $f (X) = v^⊤ h_T$ . Let $L(X, y; θ) = L(f (X), y; θ)$ be the logistic loss function for the given labeled data point $(X, y)$ and with parameters $θ = (W, U, v)$. Then, check the gradients $\\frac{\\partial L}{\\partial W}$ and $\\frac{\\partial L}{\\partial U}$. A critical term in the expression of the gradients that determines whether it explodes/vanishes will be: $$M(U) = \\prod_{k=t}^{T-1} (αU^T D_{k+1} + βI)$$ where $D_k = diag(σ′ (Wx_k + Uh_{k−1} + b))$ is the Jacobian matrix of the pointwise nonlinearity. **Hint: Analyze the range of possible values for the condition number $\\kappa$ of the matrix $M(U)$**. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **(5 points) Subtask 3: Explain why this result is useful in the context of embedded real-time applications. Comment on representation power, memory-usage, and computation.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. References & Misc. (15 points) \n",
    "\n",
    "**(a.) How many hours (approx) did you spend on this assignment totally? Document and explain your journey, including the issues/challenges you faced (both technical and non-technical).**\n",
    "\n",
    "**(b.) Cite all the resources and references you consulted to complete this assignment.** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
